{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To set up tracing to LangSmith with @traceable, first ensure that the LANGSMITH_TRACING environment variable is set to 'true'. Then, decorate any function you want to trace with @traceable in your Python code. Don't forget to set the LANGSMITH_API_KEY environment variable with your API key for authentication.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app import langsmith_rag\n",
    "\n",
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to evaluate this particular step of our application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithConflictError",
     "evalue": "Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\langsmith\\utils.py:159\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\requests\\models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\langsmith\\client.py:937\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    931\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.session.request(\n\u001b[32m    932\u001b[39m         method,\n\u001b[32m    933\u001b[39m         _construct_url(\u001b[38;5;28mself\u001b[39m.api_url, pathname),\n\u001b[32m    934\u001b[39m         stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    935\u001b[39m         **request_kwargs,\n\u001b[32m    936\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m \u001b[43mls_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status_with_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\langsmith\\utils.py:161\u001b[39m, in \u001b[36mraise_for_status_with_text\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(\u001b[38;5;28mstr\u001b[39m(e), response.text) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mHTTPError\u001b[39m: [Errno 409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets] {\"detail\":\"Dataset with this name already exists.\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLangSmithConflictError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m dataset_name = \u001b[33m\"\u001b[39m\u001b[33mTechnical Questions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m dataset = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTechnical questions about LangSmith\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Prepare inputs and outputs\u001b[39;00m\n\u001b[32m     30\u001b[39m inputs = [{\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: q, \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: c} \u001b[38;5;28;01mfor\u001b[39;00m q, c, _ \u001b[38;5;129;01min\u001b[39;00m example_dataset]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\langsmith\\client.py:3807\u001b[39m, in \u001b[36mClient.create_dataset\u001b[39m\u001b[34m(self, dataset_name, description, data_type, inputs_schema, outputs_schema, transformations, metadata)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m outputs_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3805\u001b[39m     dataset[\u001b[33m\"\u001b[39m\u001b[33moutputs_schema_definition\u001b[39m\u001b[33m\"\u001b[39m] = outputs_schema\n\u001b[32m-> \u001b[39m\u001b[32m3807\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3808\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3809\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/datasets\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mContent-Type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_orjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3812\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m ls_utils.raise_for_status_with_text(response)\n\u001b[32m   3815\u001b[39m json_response = response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Saket-langsmith-MAT496\\langenv\\Lib\\site-packages\\langsmith\\client.py:982\u001b[39m, in \u001b[36mClient.request_with_retries\u001b[39m\u001b[34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[39m\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithNotFoundError(\n\u001b[32m    978\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResource not found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    979\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    980\u001b[39m     )\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ls_utils.LangSmithConflictError(\n\u001b[32m    983\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConflict for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    984\u001b[39m     )\n\u001b[32m    985\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    986\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mLangSmithConflictError\u001b[39m: Conflict for /datasets. HTTPError('409 Client Error: Conflict for url: https://api.smith.langchain.com/datasets', '{\"detail\":\"Dataset with this name already exists.\"}')"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = [\n",
    "    (\n",
    "        \"How do I set up tracing to LangSmith with @traceable?\",\n",
    "        \"\"\" 2. Log a trace‚Äã\\nOnce you've set up your environment, you can call LangChain runnables as normal.\\nLangSmith will infer the proper tracing config:\\n\\nHow to log and view traces to LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n2. Log a trace‚Äã\\nOnce you've set up your environment, wrap or decorate the custom functions/SDKs you want to trace.\\nLangSmith will then infer the proper tracing config:\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingAnnotate code for tracingOn this pageAnnotate code for tracing\\nThere are several ways to log traces to LangSmith.\\ntipIf you are using LangChain (either Python or JS/TS), you can skip this section and go directly to the LangChain-specific instructions.\\nUse @traceable / traceable‚Äã\\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable decorator in Python and traceable function in TypeScript.\\nnoteThe LANGSMITH_TRACING environment variable must be set to 'true' in order for traces to be logged to LangSmith, even when using @traceable or traceable. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY environment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default.\\nTo log traces to a different project, see this section. \\n\\n\"\"\",\n",
    "        \"To set up tracing to LangSmith using the `@traceable` decorator in Python, first ensure that you have the `LANGSMITH_TRACING` environment variable set to 'true' and the `LANGSMITH_API_KEY` set to your API key. Then, you can simply decorate your functions like this:\\n\\n```python\\nfrom langsmith import traceable\\n\\n@traceable\\ndef my_function():\\n    # Your code here\\n    pass\\n```\\n\\nThis will log traces for `my_function` automatically once the necessary environment variables are configured.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use the tracing context manager?\",\n",
    "        \"\"\"Use the trace context manager (Python only)‚Äã\\nIn Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\\n\\nYou want to log traces for a specific block of code.\\nYou want control over the inputs, outputs, and other attributes of the trace.\\nIt is not feasible to use a decorator or wrapper.\\nAny or all of the above.\\n\\nIn some environments, it is not possible to set environment variables. In these cases, you can set the tracing configuration programmatically.\\nRecently changed behaviorDue to a number of asks for finer-grained control of tracing using the trace context manager,\\nwe changed the behavior of with trace to honor the LANGSMITH_TRACING environment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes.\\nThe recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context context manager, as shown in the example below.\\nPythonTypeScriptThe recommended way to do this in Python is to use the tracing_context context manager. This works for both code annotated with traceable and code within the trace context manager.\\n\\nSkip to main contentGo to API DocsSearchRegionUSEUGo to AppQuick startObservabilityConceptual GuideHow-to GuidesTracingAnnotate code for tracingToggle tracing on and offUpload files with tracesLog traces to specific projectSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataTrace JS functions in serverless environmentsMonitoring and automationsTutorialsAdd observability to your LLM applicationEvaluationPrompt EngineeringDeployment (LangGraph Cloud)AdministrationSelf-hostingReferenceObservabilityHow-to GuidesTracingTroubleshoot trace nestingOn this pageTroubleshoot trace nesting\\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.\\nIf you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \\\"edge cases\\\".\\nPython‚Äã\\nThe following outlines common causes for \\\"split\\\" traces when building with python.\\nContext propagation using asyncio‚Äã\\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python's asyncio only added full support for passing context in version 3.11.\\nWhy‚Äã\\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\\nTo resolve‚Äã\\n\\nContext propagation using threading‚Äã\\nIt's common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python's stdlib ThreadPoolExecutor by default breaks tracing.\\nWhy‚Äã\\nPython's contextvars start empty within new threads. Here are two approaches to handle maintain trace contiguity:\\nTo resolve‚Äã\\n\\n\\nUsing LangSmith's ContextThreadPoolExecutor\\nLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func():    with ContextThreadPoolExecutor() as executor:        inputs = [1, 2]        r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x):    print(x)outer_func()\\n\\n\\nManually providing the parent run tree\\nAlternatively, you can manually pass the parent run tree to the inner function:\\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func():    rt = get_current_run_tree()    with ThreadPoolExecutor() as executor:        r = list(            executor.map(                lambda x: inner_func(x, langsmith_extra={\\\"parent\\\": rt}), [1, 2]            )        )@traceabledef inner_func(x):    print(x)outer_func()\\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter. \\n\\n \"\"\",\n",
    "        \"You can use the tracing context manager by wrapping the code you want to trace with the `with trace_context:` statement. Here's a simple example:\\n\\n```python\\nfrom langsmith import trace_context\\n\\nwith trace_context:\\n    # Your traceable code here\\n    print(\\\"Tracing this block of code.\\\")\\n``` \\n\\nThis will log traces to LangSmith for the specified code block.\"\n",
    "    ),\n",
    "    (\n",
    "        \"How can I use RunTree?\",\n",
    "        \"\"\"PythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-4o-mini\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});await pipeline.postRun();// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI Call\\\",\\n\\nPythonTypeScriptimport openaifrom langsmith.run_trees import RunTree# This can be a user input to your appquestion = \\\"Can you summarize this morning's meetings?\\\"# Create a top-level runpipeline = RunTree(  name=\\\"Chat Pipeline\\\",  run_type=\\\"chain\\\",  inputs={\\\"question\\\": question})await pipeline.postRun();# This can be retrieved in a retrieval stepcontext = \\\"During this morning's meeting, we solved all world conflict.\\\"messages = [  { \\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { \\\"role\\\": \\\"user\\\", \\\"content\\\": f\\\"Question: {question}\\\\nContext: {context}\\\"}]# Create a child runchild_llm_run = pipeline.create_child(  name=\\\"OpenAI Call\\\",  run_type=\\\"llm\\\",  inputs={\\\"messages\\\": messages},)# Generate a completionclient = openai.Client()chat_completion = client.chat.completions.create(  model=\\\"gpt-3.5-turbo\\\", messages=messages)# End the runs and log themchild_llm_run.end(outputs=chat_completion)child_llm_run.postRun()pipeline.end(outputs={\\\"answer\\\": chat_completion.choices[0].message.content})pipeline.postRun()import OpenAI from \\\"openai\\\";import { RunTree } from \\\"langsmith\\\";// This can be a user input to your appconst question = \\\"Can you summarize this morning's meetings?\\\";const pipeline = new RunTree({  name: \\\"Chat Pipeline\\\",  run_type: \\\"chain\\\",  inputs: { question }});// This can be retrieved in a retrieval stepconst context = \\\"During this morning's meeting, we solved all world conflict.\\\";const messages = [  { role: \\\"system\\\", content: \\\"You are a helpful assistant. Please respond to the user's request only based on the given context.\\\" },  { role: \\\"user\\\", content: `Question: ${question}Context: ${context}` }];// Create a child runconst childRun = await pipeline.createChild({  name: \\\"OpenAI\\n\\nAlternatively, you can convert LangChain's RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable-wrapped function.\\n\\nRunTree({  run_type: \\\"llm\\\",  name: \\\"OpenAI Call RunTree\\\",  inputs: { messages },  tags: [\\\"my-tag\\\"],  extra: {metadata: {\\\"my-key\\\": \\\"my-value\\\"}}})await rt.postRun();const chatCompletion = await client.chat.completions.create({  model: \\\"gpt-3.5-turbo\\\",  messages: messages,});// End and submit the runawait rt.end(chatCompletion)await rt.patchRun()from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserprompt = ChatPromptTemplate.from_messages([(\\\"system\\\", \\\"You are a helpful AI.\\\"),(\\\"user\\\", \\\"{input}\\\")])chat_model = ChatOpenAI()output_parser = StrOutputParser()# Tags and metadata can be configured with RunnableConfigchain = (prompt | chat_model | output_parser).with_config({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}})# Tags and metadata can also be passed at runtimechain.invoke({\\\"input\\\": \\\"What is the meaning of life?\\\"}, {\\\"tags\\\": [\\\"shared-tags\\\"], \\\"metadata\\\": {\\\"shared-key\\\": \\\"shared-value\\\"}})import { ChatOpenAI } from \\\"@langchain/openai\\\";import { ChatPromptTemplate } from \\\"@langchain/core/prompts\\\";import { StringOutputParser } from \\\"@langchain/core/output_parsers\\\";const prompt = ChatPromptTemplate.fromMessages([[\\\"system\\\", \\\"You are a helpful AI.\\\"],[\\\"user\\\", \\\"{input}\\\"]])const model = new ChatOpenAI({ modelName: \\\"gpt-3.5-turbo\\\" });const outputParser = new StringOutputParser();// Tags and metadata can be configured with RunnableConfigconst chain = (prompt.pipe(model).pipe(outputParser)).withConfig({\\\"tags\\\": [\\\"top-level-tag\\\"], \\\"metadata\\\": {\\\"top-level-key\\\": \\\"top-level-value\\\"}});// Tags and metadata can also be \\n\\n\"\"\",\n",
    "        \"You can use `RunTree` by creating a pipeline where you define its name, type, and inputs. For example:\\n\\n```python\\nfrom langsmith.run_trees import RunTree\\n\\npipeline = RunTree(name=\\\"My Pipeline\\\", run_type=\\\"chain\\\", inputs={\\\"question\\\": \\\"Your question here\\\"})\\n```\\n\\nYou can then create child runs and manage them accordingly within the pipeline.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Technical Questions\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Technical questions about LangSmith\"\n",
    ")\n",
    "\n",
    "# Prepare inputs and outputs\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "# Create examples in the dataset\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update our Application to use Prompt Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to pretty much define the same RAG application as before - with one crucial improvement.\n",
    "\n",
    "Instead of pulling our `RAG_PROMPT` from utils.py, we're going to connect to the Prompt Hub in LangSmith.\n",
    "\n",
    "Let's add the code snippet that will pull down our prompt that we just iterated on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "prompt = client.pull_prompt(\"rag_with_code:cdcbce89\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "# TODO: Remove this hard-coded prompt and replace it with Prompt Hub\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "\n",
    "    # If vector store exists, then load it\n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    # Otherwise, index LangSmith documents and create new vector store\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    formatted_prompt = prompt.invoke({\"context\":formatted_docs, \"question\": question})\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do I set up tracing to LangSmith with @traceable?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_dataset = attack_on_titan_dataset = [\n",
    "    (\"How do rockets launch into space?\", \n",
    "     \"Rockets launch into space using powerful engines that overcome Earth's gravity\", \n",
    "     \"Answer about rocket launches\"),\n",
    "    \n",
    "    (\"Is the Moon a planet?\", \n",
    "     \"No, the Moon is a natural satellite of Earth\", \n",
    "     \"Answer about the Moon\"),\n",
    "    \n",
    "    (\"Who was the first human in space?\", \n",
    "     \"Yuri Gagarin was the first human to travel into space\", \n",
    "     \"Answer about first human in space\"),\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"space_queries_dataset\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Space exploration related queries\"\n",
    ")\n",
    "\n",
    "inputs = [{\"question\": q, \"context\": c} for q, c, _ in example_dataset]\n",
    "outputs = [{\"output\": o} for _, _, o in example_dataset]\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337: UserWarning: WARNING! extra_headers is not default parameter.\n",
      "                extra_headers was transferred to model_kwargs.\n",
      "                Please confirm that extra_headers is what you intended.\n",
      "  obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n"
     ]
    }
   ],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"space_queries\", include_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langsmith import traceable\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "APP_VERSION = 1.0\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "# Pull just the prompt template without model binding\n",
    "prompt = client.pull_prompt(\"space_queries\")  # No include_model\n",
    "\n",
    "def get_vector_db_retriever():\n",
    "    persist_path = os.path.join(tempfile.gettempdir(), \"union.parquet\")\n",
    "    embd = OpenAIEmbeddings()\n",
    "    \n",
    "    if os.path.exists(persist_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=embd,\n",
    "            persist_path=persist_path,\n",
    "            serializer=\"parquet\"\n",
    "        )\n",
    "        return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\", continue_on_failure=True)\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=embd,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\"\n",
    "    )\n",
    "    vectorstore.persist()\n",
    "    return vectorstore.as_retriever(lambda_mult=0)\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str):\n",
    "    formatted_prompt = prompt.invoke({\"question\": question})\n",
    "    messages = convert_prompt_to_openai_format(formatted_prompt)[\"messages\"]\n",
    "    return call_openai(messages)\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_openai(messages: List[dict]) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    response = generate_response(question)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first human in space was Yuri Gagarin, a Soviet cosmonaut. He completed his historic flight on April 12, 1961, aboard the Vostok 1 spacecraft.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who was the first human in space?\"\n",
    "langsmith_rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langenv)",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
