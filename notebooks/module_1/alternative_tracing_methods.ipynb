{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Tracing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
    "\n",
    "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain.schema import Document\n",
    "from langsmith import traceable\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
    "\n",
    "You can also pass in metadata or other fields through an optional config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How do I set up tracing if I'm using LangChain?\",\n",
       " 'messages': [HumanMessage(content=\"How do I set up tracing if I'm using LangChain?\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='To set up tracing with LangChain, you need to configure your environment by setting a few environment variables. After that, you can log a trace by following the instructions in the Trace With LangChain guide. For detailed steps, refer to the documentation provided by LangChain.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 1398, 'total_tokens': 1452, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_51db84afab', 'id': 'chatcmpl-CN5Wp9hRJ4gpKumwGYJ5y7g3GNXR5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--867e1f45-10be-420b-a1b6-e4b91864ba7a-0', usage_metadata={'input_tokens': 1398, 'output_tokens': 54, 'total_tokens': 1452, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'documents': [Document(metadata={'id': 'c115db0d-f4d0-44d0-99c0-59936669cff3', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry'}, page_content='Set up distributed tracing with LangChain\\u200b\\nTo enable distributed tracing across multiple services:'),\n",
       "  Document(metadata={'id': '635788c1-3681-4e98-a154-69aed3d88daf', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph'}, page_content='Trace with LangGraph - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangGraphGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWith LangChain1. Installation2. Configure your environment3. Log a traceWithout LangChain1. Installation2. Configure your environment3. Log a traceTracing setupIntegrationsTrace with LangGraphCopy pageCopy pageLangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agentic workflows, whether you’re using LangChain modules or other SDKs.\\n\\u200bWith LangChain\\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\\nThis guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.\\n\\u200b1. Installation'),\n",
       "  Document(metadata={'id': '1925c298-348b-465d-a81e-d8e317843f65', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain'),\n",
       "  Document(metadata={'id': '00aaee64-2463-48d1-ad99-c4361b9fb111', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"How do I set up tracing if I'm using LangChain?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why is tracing useful in Langchain?',\n",
       " 'messages': [HumanMessage(content='Why is tracing useful in Langchain?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Tracing in LangChain is useful for observability and evaluation of large language model (LLM) workflows, allowing developers to monitor and analyze the performance of their applications. It helps in identifying bottlenecks, debugging issues, and optimizing the overall system by providing insights into the execution flow. Additionally, tracing facilitates better collaboration and understanding of complex interactions between different services.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 1394, 'total_tokens': 1466, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CN5XviVJz2jahegMvqU5824Q1YSvA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2244660e-4beb-4741-907f-4670d74ba30c-0', usage_metadata={'input_tokens': 1394, 'output_tokens': 72, 'total_tokens': 1466, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'documents': [Document(metadata={'id': 'c115db0d-f4d0-44d0-99c0-59936669cff3', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry'}, page_content='Set up distributed tracing with LangChain\\u200b\\nTo enable distributed tracing across multiple services:'),\n",
       "  Document(metadata={'id': '1925c298-348b-465d-a81e-d8e317843f65', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain'),\n",
       "  Document(metadata={'id': '00aaee64-2463-48d1-ad99-c4361b9fb111', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides'}, page_content='Trace with LangChain (Python and JS/TS) - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangChain (Python and JS/TS)Get startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageInstallationQuick start1. Configure your environment2. Log a trace3. View your traceTrace selectivelyLog to a specific projectStaticallyDynamicallyAdd metadata and tags to tracesCustomize run nameCustomize run IDAccess run (span) ID for LangChain invocationsEnsure all traces are submitted before exitingTrace without setting environment variablesDistributed tracing with LangChain (Python)Interoperability between LangChain (Python) and LangSmith SDKInteroperability between LangChain.JS and LangSmith SDKTracing LangChain objects inside traceable (JS only)Tracing LangChain child runs via traceable / RunTree API (JS only)Tracing setupIntegrationsTrace with LangChain'),\n",
       "  Document(metadata={'id': '635788c1-3681-4e98-a154-69aed3d88daf', 'changefreq': 'weekly', 'loc': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph', 'priority': '0.5', 'source': 'https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph'}, page_content='Trace with LangGraph - Docs by LangChainSkip to main contentOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewConceptsTutorial - Trace a RAG applicationTracing setupIntegrationsOverviewLangChainLangGraphAnthropic (Python only)OpenAIAutoGenClaude CodeCrewAIGoogle ADKInstructor (Python only)OpenAI Agents SDKOpenTelemetrySemantic KernelVercel AI SDKManual instrumentationConfiguration & troubleshootingProject & environment settingsAdvanced tracing techniquesData & privacyTroubleshooting guidesViewing & managing tracesFilter tracesQuery traces (SDK)Compare tracesShare or unshare a trace publiclyView server logs for a traceBulk export trace dataAutomationsSet up automation rulesConfigure webhook notifications for rulesFeedback & evaluationLog user feedback using the SDKSet up online evaluatorsMonitoring & alertingMonitor projects with dashboardsAlertsConfigure webhook notifications for alertsInsights (Beta)Data type referenceRun (span) data formatFeedback data formatTrace query syntaxOur new LangChain Academy course on Deep Agents is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KGitHubForumForumSearch...NavigationIntegrationsTrace with LangGraphGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGitHubForumOn this pageWith LangChain1. Installation2. Configure your environment3. Log a traceWithout LangChain1. Installation2. Configure your environment3. Log a traceTracing setupIntegrationsTrace with LangGraphCopy pageCopy pageLangSmith smoothly integrates with LangGraph (Python and JS) to help you trace agentic workflows, whether you’re using LangChain modules or other SDKs.\\n\\u200bWith LangChain\\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing.\\nThis guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.\\n\\u200b1. Installation')]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Why is tracing useful in Langchain?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
    "\n",
    "You want to log traces for a specific block of code.\n",
    "You want control over the inputs, outputs, and other attributes of the trace.\n",
    "It is not feasible to use a decorator or wrapper.\n",
    "Any or all of the above.\n",
    "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, trace\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    with trace(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "        metadata={\"foo\": \"bar\"},\n",
    "    ):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": RAG_SYSTEM_PROMPT\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "            }\n",
    "        ]\n",
    "        response = call_openai(messages)\n",
    "    return response\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_openai(\n",
    "  messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with the tracing context in Python, you can use the `tracing_context` context manager provided by LangSmith. Wrap your code that you want to trace within the `with ls.tracing_context(enabled=True):` block to enable tracing for that specific invocation. This allows you to selectively trace parts of your application without relying on environment variables.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with tracing context?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase: LANGSMITH DEMO, Reversed: OMED HTIMSGNAL\n"
     ]
    }
   ],
   "source": [
    "from langsmith import trace\n",
    "\n",
    "def to_uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "def reverse_string(text: str) -> str:\n",
    "    return text[::-1]\n",
    "\n",
    "def string_workflow(text: str):\n",
    "    with trace(\n",
    "        name=\"String Workflow\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"text\": text},\n",
    "        metadata={\"purpose\": \"demo of trace context manager with strings\"}\n",
    "    ) as t:\n",
    "        upper = to_uppercase(text)\n",
    "        reversed_text = reverse_string(upper)\n",
    "\n",
    "        t.end(outputs={\"uppercase\": upper, \"reversed\": reversed_text})\n",
    "\n",
    "    return upper, reversed_text\n",
    "\n",
    "upper_val, reversed_val = string_workflow(\"LangSmith Demo\")\n",
    "print(f\"Uppercase: {upper_val}, Reversed: {reversed_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with the tracing context in Python, you can use the `tracing_context` context manager from the LangSmith SDK. Wrap your code that you want to trace within `with ls.tracing_context(enabled=True):` to enable tracing for that specific invocation. This allows you to selectively trace parts of your application without relying on environment variables.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with tracing context?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = openai.Client()\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=messages,\n",
    "    )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_openai(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with `wrap_openai`, you need to wrap your OpenAI client using the `wrap_openai` method from `langsmith.wrappers`. Ensure that the environment variable `LANGSMITH_TRACING` is set to 'true' and provide your LangSmith API key by setting the `LANGSMITH_API_KEY` environment variable. You can also specify the workspace with `LANGSMITH_WORKSPACE_ID` if your API key is linked to multiple workspaces.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with wrap_openai?\"\n",
    "ai_answer = langsmith_rag_with_wrap_openai(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      " The **Mona Lisa**, painted by Leonardo da Vinci in the early 16th century, holds a pivotal place in art history for several reasons:\n",
      "\n",
      "1. **Innovative Techniques**: Leonardo employed groundbreaking techniques such as sfumato, which creates a soft transition between colors and tones, enhancing the realism of the subject. This technique influenced countless artists and shifted the approach to portrait painting.\n",
      "\n",
      "2. **Psychological Depth**: The Mona Lisa is renowned for its enigmatic expression and the sense of individuality it conveys. Da Vinci's ability to capture the subject's emotions and personality marked a shift towards greater psychological depth in portraiture, moving away from the more rigid and formal representations of earlier periods.\n",
      "\n",
      "3. **Composition and Use of Space**: The composition \n",
      "\n",
      "Response 2:\n",
      " Python is a high-level, interpreted programming language known for its readability and versatility. Here are some key features:\n",
      "\n",
      "1. **Readability**: Python’s syntax is clean and easy to understand, which promotes readability and reduces the cost of program maintenance.\n",
      "\n",
      "2. **Interpreted Language**: Python is executed line by line, which makes debugging easier and allows for interactive coding.\n",
      "\n",
      "3. **Dynamic Typing**: Variables in Python do not require an explicit declaration to reserve memory space; the declaration happens automatically when a value is assigned.\n",
      "\n",
      "4. **Extensive Standard Library**: Python comes with a vast standard library that supports many tasks, such as file I/O, system calls, and even web development.\n",
      "\n",
      "5. **Object-Oriented**\n"
     ]
    }
   ],
   "source": [
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "wrapped_client = wrap_openai(client)\n",
    "\n",
    "def ask_openai(prompt: str):\n",
    "    \"\"\"Function that sends a prompt to the wrapped OpenAI client and returns response.\"\"\"\n",
    "    response = wrapped_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # You can change to gpt-4, gpt-4o, etc.\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    answer1 = ask_openai(\"Explain the significance of the Mona Lisa painting in art history.\")\n",
    "    print(\"Response 1:\\n\", answer1, \"\\n\")\n",
    "\n",
    "    answer2 = ask_openai(\"Summarize the key features of Python programming language.\")\n",
    "    print(\"Response 2:\\n\", answer2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CN5ijaCvXZbbWQbLLBQTjw5Oib8tp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The color of the sky can vary depending on the time of day, weather, and atmospheric conditions. During a clear day, the sky typically appears blue due to the scattering of sunlight by the Earth's atmosphere. Near sunrise and sunset, the sky can take on shades of orange, pink, and purple. On cloudy or overcast days, the sky may appear gray. Additionally, during events like storms, the sky can take on darker hues.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1759619569, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_51db84afab', usage=CompletionUsage(completion_tokens=88, prompt_tokens=13, total_tokens=101, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What color is the sky?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    metadata={\"foo\": \"bar\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped OpenAI client accepts all the same langsmith_extra parameters as @traceable decorated functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced] RunTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I have my env variables defined in a .env file\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and set `LANGSMITH_TRACING` to false, as we are using RunTree to manually create runs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled() # This should return false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    openai_response = call_openai(child_run, messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response\n",
    "\n",
    "def call_openai(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"OpenAI Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    openai_response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    # Create a root RunTree\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    # Pass our RunTree into the nested function calls\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    # Post our final output\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated: LangSmith makes tracing easy, Word Count: 4\n",
      "\n",
      "RunTree trace:\n",
      "Text Pipeline Demo\n",
      "  inputs: {'strings': ['LangSmith', 'makes', 'tracing', 'easy']}\n",
      "  outputs: {'concatenated': 'LangSmith makes tracing easy', 'word_count': 4}\n"
     ]
    }
   ],
   "source": [
    "from langsmith import RunTree\n",
    "\n",
    "def concatenate_strings(parent_run: RunTree, strings):\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Concatenate Strings\",\n",
    "        run_type=\"tool\",\n",
    "        inputs={\"strings\": strings}\n",
    "    )\n",
    "    result = \" \".join(strings)\n",
    "    child_run.end(outputs={\"concatenated\": result})\n",
    "    child_run.post()\n",
    "    return result\n",
    "\n",
    "def count_words(parent_run: RunTree, text):\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Count Words\",\n",
    "        run_type=\"tool\",\n",
    "        inputs={\"text\": text}\n",
    "    )\n",
    "    count = len(text.split())\n",
    "    child_run.end(outputs={\"word_count\": count})\n",
    "    child_run.post()\n",
    "    return count\n",
    "\n",
    "def text_pipeline(strings):\n",
    "    root_run = RunTree(name=\"Text Pipeline Demo\", run_type=\"chain\", inputs={\"strings\": strings})\n",
    "    concatenated = concatenate_strings(root_run, strings)\n",
    "    word_count = count_words(root_run, concatenated)\n",
    "    root_run.end(outputs={\"concatenated\": concatenated, \"word_count\": word_count})\n",
    "    root_run.post()\n",
    "    return concatenated, word_count, root_run\n",
    "\n",
    "def print_run_tree(run: RunTree, indent=0):\n",
    "    spacer = \" \" * indent\n",
    "    print(f\"{spacer}{run.name}\")\n",
    "    if run.inputs:\n",
    "        print(f\"{spacer}  inputs: {run.inputs}\")\n",
    "    if run.outputs:\n",
    "        print(f\"{spacer}  outputs: {run.outputs}\")\n",
    "    for child in getattr(run, \"children\", []):\n",
    "        print_run_tree(child, indent + 4)\n",
    "\n",
    "strings = [\"LangSmith\", \"makes\", \"tracing\", \"easy\"]\n",
    "concatenated, word_count, run_tree = text_pipeline(strings)\n",
    "\n",
    "print(f\"Concatenated: {concatenated}, Word Count: {word_count}\\n\")\n",
    "print(\"RunTree trace:\")\n",
    "print_run_tree(run_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with RunTree, you can use the `RunTree.fromHeaders` method to create a run tree from the request headers. Then, utilize the `withRunTree` helper to ensure the run tree is propagated within traceable invocations. This allows you to maintain the context of the trace throughout your application's operations.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langenv)",
   "language": "python",
   "name": "langenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
